# -*- coding: utf-8 -*-
"""Locate_segment_text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14F4bS9T2HtVxGlDTGHE9W4yMPYACPVQH
"""

from google.colab import drive 
drive.mount ('/content/drive')

import pandas as pd 
dataset= pd.read_csv ('/content/drive/MyDrive/New_dataset.csv', encoding= 'ISO-8859-1')

dataset.head()

dataset['Article'][2]

# first of all lets use RNN, lets see if it works!

dataset.isnull().sum() #checking if there is any null value

dataset.shape #The shape of the dataset

x= dataset['Article'].copy()

x.head()

y= dataset[['is_deadline', 'start', 'end']].copy()

y.head()

x.shape, y.shape

x[0]

#Predicting using LSTM

from tensorflow.keras.layers import Embedding 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.preprocessing.text import one_hot 
from tensorflow.keras.layers import LSTM 
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

#Vocabulary size 
voc_size=5000

import nltk 
import re 
from nltk.corpus import stopwords

nltk.download ('stopwords')

from nltk.stem.porter import PorterStemmer 

ps= PorterStemmer () 
corpus=[] 
for i in range (0, len (x)): 
  review= re.sub ('[^a-zA-z]', ' ', x[i])
  review= review.lower () 
  review= review.split ()
  review= [ps.stem (word) for word in review if not word in stopwords.words ('english')] 
  review= ' '.join (review) 
  corpus.append (review)

corpus[0]

one_hot_representation_1= [one_hot (words, voc_size) for words in corpus]

#Embedding layer 
sent_length=150 
embedding_docs_1= pad_sequences (one_hot_representation_1, padding='pre', maxlen=sent_length)

embedding_docs_1[1]

embedding_vector_features=100 
model_1= Sequential () 
model_1.add(Embedding (voc_size, embedding_vector_features, input_length= sent_length)) 
model_1.add(LSTM (100)) 
model_1.add(Dense (100, activation='relu'))
model_1.add (Dense (3))

model_1.compile (optimizer= 'adam', loss= 'mse', metrics= ['mae'])

model_1.summary()

import numpy as np
x_final_1= np.array (embedding_docs_1) 
y_final= np.array (y)

y_final[1]

x_final_1.shape, y_final.shape

history= model_1.fit (x_final_1, y_final, validation_split=0.2, epochs=500, batch_size=64, verbose=2)

np.set_printoptions(formatter={'float_kind':'{:f}'.format})

y_pred= model_1.predict (x_final_1)

y_pred[0:5], y_final[0:5]

# Applying other algorithms 
def clean_data (s): 
  s= str (s) 
  s= s.lower () 
  s= re.sub ('\s\W', ' ',s ) # \s means unicode whitespace character and \W is equivalent to [^a-zA-Z0-9]
  s= re.sub ('\W,\s', ' ',s)  
  s = re.sub(r'[^\w]', ' ', s)
  s = re.sub("\d+", "", s)
  s = re.sub('\s+',' ',s)
  s = re.sub('[!@#$_]', '', s)
  s = s.replace("co","")
  s = s.replace("https","")
  s = s.replace(",","")
  s = s.replace("[\w*"," ")
  return s

x_clean= x.apply (clean_data)

x_clean [3]

from sklearn.feature_extraction.text import CountVectorizer

vectorizer= CountVectorizer() 
x_vectorizer= vectorizer.fit_transform (x)

x_vectorizer

# K nearest Neighbor regression

from sklearn.neighbors import KNeighborsRegressor

model_knn= KNeighborsRegressor ()

model_knn.fit (x_vectorizer, y_final)

y_knn= model_knn.predict (x_vectorizer)

y_knn[0:10], y_final[0:10]

#Support vector regressor

from sklearn.svm import SVR

model_svr= SVR(epsilon=0.2)

from sklearn.multioutput import MultiOutputRegressor

mor= MultiOutputRegressor(model_svr)

mor.fit (x_vectorizer, y_final)

y_svr= mor.predict (x_vectorizer)

y_svr[0:5], y_final[0:5]

# XGBoost Regressor. Ensemble techniques are powerful than other ML algorithms. So here XGBoost Regression algorithm is used to predict the targets.

!pip install xgboost

from xgboost import XGBRegressor

model_xg= XGBRegressor()

xg= MultiOutputRegressor (model_xg)

xg.fit (x_vectorizer, y_final)

y_xg= xg.predict (x_vectorizer)

y_xg[0:10], y_final[0:10]

# Among these techniques, LSTM provides slightly better result but needs to be trained more. Also LSTM has overfitting problem but still it can predict the targets better than other techniques.